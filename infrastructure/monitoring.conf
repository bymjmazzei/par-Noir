# Monitoring and Alerting Configuration for Production Identity Protocol
# This file configures monitoring, alerting, and incident response

## Prometheus Configuration (prometheus.yml)
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'identity-protocol-api'
    static_configs:
      - targets: ['localhost:3001']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['localhost:9121']

  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['localhost:9187']

## Alert Rules (alert_rules.yml)
groups:
  - name: identity-protocol
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% for more than 5 minutes"

      # High memory usage
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% for more than 5 minutes"

      # High disk usage
      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is above 85% for more than 5 minutes"

      # API response time
      - alert: HighAPIResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High API response time"
          description: "95th percentile response time is above 2 seconds"

      # High error rate
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for more than 5 minutes"

      # Authentication failures
      - alert: HighAuthFailures
        expr: rate(auth_failures_total[5m]) > 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High authentication failure rate"
          description: "More than 10 authentication failures per minute"

      # Database connection issues
      - alert: DatabaseConnectionIssues
        expr: up{job="postgres-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection issues"
          description: "Database exporter is down"

      # Redis connection issues
      - alert: RedisConnectionIssues
        expr: up{job="redis-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis connection issues"
          description: "Redis exporter is down"

## Grafana Dashboard Configuration
# Create dashboards for:
# - System metrics (CPU, memory, disk, network)
# - Application metrics (API response time, error rate, throughput)
# - Security metrics (authentication failures, blocked requests)
# - Business metrics (active users, transactions, data points)

## Alert Manager Configuration (alertmanager.yml)
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'slack-notifications'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        actions:
          - type: button
            text: 'View in Grafana'
            url: '{{ template "slack.grafana" . }}'

## Log Aggregation Configuration
# Use ELK Stack (Elasticsearch, Logstash, Kibana) or similar
# Configure log shipping for:
# - Application logs
# - System logs
# - Security logs
# - Access logs

## Incident Response Procedures
# 1. Alert Classification
#    - Critical: Immediate response required
#    - Warning: Response within 1 hour
#    - Info: Monitor and log

# 2. Escalation Matrix
#    - Level 1: On-call engineer
#    - Level 2: Senior engineer
#    - Level 3: Engineering manager
#    - Level 4: CTO/VP Engineering

# 3. Response Times
#    - Critical: 15 minutes
#    - Warning: 1 hour
#    - Info: 4 hours

## Monitoring Checklist
# - [ ] Prometheus metrics collection configured
# - [ ] Alert rules defined and tested
# - [ ] Alert manager configured with notifications
# - [ ] Grafana dashboards created
# - [ ] Log aggregation system configured
# - [ ] Incident response procedures documented
# - [ ] On-call rotation established
# - [ ] Monitoring coverage verified
# - [ ] Alert thresholds tuned
# - [ ] False positive rate minimized
